{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping Lab\n",
    "\n",
    "In this lab you will first learn the following code snippet which is a simple web spider class that allows you to scrape paginated webpages. Read the code, run it, and make sure you understand how it work. In the challenges of this lab, we will guide you in building up this class so that eventually you will have a more robus web spider that you can further work on in the Web Scraping Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<title>Quotes to Scrape</title>\\n    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\\n    <link rel=\"stylesheet\" href=\"/static/main.css\">\\n</head>\\n<body>\\n    <div class=\"container\">\\n        <div class=\"row header-box\">\\n            <div class=\"col-md-8\">\\n                <h1>\\n                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\\n                </h1>\\n            </div>\\n            <div class=\"col-md-4\">\\n                <p>\\n                \\n                    <a href=\"/login\">Login</a>\\n                \\n                </p>\\n            </div>\\n        </div>\\n    \\n\\n<div class=\"row\">\\n    <div class=\"col-md-8\">\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\\n        <a href=\"/author/Albert-Einstein\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\\n            \\n            <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\\n            \\n            <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\\n            \\n            <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cIt is our choices, Harry, that show what we truly are, far more than our abilities.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">J.K. Rowling</small>\\n        <a href=\"/author/J-K-Rowling\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"abilities,choices\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/abilities/page/1/\">abilities</a>\\n            \\n            <a class=\"tag\" href=\"/tag/choices/page/1/\">choices</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\\n        <a href=\"/author/Albert-Einstein\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"inspirational,life,live,miracle,miracles\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\\n            \\n            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\\n            \\n            <a class=\"tag\" href=\"/tag/live/page/1/\">live</a>\\n            \\n            <a class=\"tag\" href=\"/tag/miracle/page/1/\">miracle</a>\\n            \\n            <a class=\"tag\" href=\"/tag/miracles/page/1/\">miracles</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Jane Austen</small>\\n        <a href=\"/author/Jane-Austen\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"aliteracy,books,classic,humor\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/aliteracy/page/1/\">aliteracy</a>\\n            \\n            <a class=\"tag\" href=\"/tag/books/page/1/\">books</a>\\n            \\n            <a class=\"tag\" href=\"/tag/classic/page/1/\">classic</a>\\n            \\n            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cImperfection is beauty, madness is genius and it&#39;s better to be absolutely ridiculous than absolutely boring.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>\\n        <a href=\"/author/Marilyn-Monroe\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"be-yourself,inspirational\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/be-yourself/page/1/\">be-yourself</a>\\n            \\n            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cTry not to become a man of success. Rather become a man of value.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\\n        <a href=\"/author/Albert-Einstein\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"adulthood,success,value\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/adulthood/page/1/\">adulthood</a>\\n            \\n            <a class=\"tag\" href=\"/tag/success/page/1/\">success</a>\\n            \\n            <a class=\"tag\" href=\"/tag/value/page/1/\">value</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cIt is better to be hated for what you are than to be loved for what you are not.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Andr\\xc3\\xa9 Gide</small>\\n        <a href=\"/author/Andre-Gide\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"life,love\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\\n            \\n            <a class=\"tag\" href=\"/tag/love/page/1/\">love</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cI have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Thomas A. Edison</small>\\n        <a href=\"/author/Thomas-A-Edison\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"edison,failure,inspirational,paraphrased\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/edison/page/1/\">edison</a>\\n            \\n            <a class=\"tag\" href=\"/tag/failure/page/1/\">failure</a>\\n            \\n            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\\n            \\n            <a class=\"tag\" href=\"/tag/paraphrased/page/1/\">paraphrased</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cA woman is like a tea bag; you never know how strong it is until it&#39;s in hot water.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Eleanor Roosevelt</small>\\n        <a href=\"/author/Eleanor-Roosevelt\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"misattributed-eleanor-roosevelt\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/misattributed-eleanor-roosevelt/page/1/\">misattributed-eleanor-roosevelt</a>\\n            \\n        </div>\\n    </div>\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cA day without sunshine is like, you know, night.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Steve Martin</small>\\n        <a href=\"/author/Steve-Martin\">(about)</a>\\n        </span>\\n        <div class=\"tags\">\\n            Tags:\\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"humor,obvious,simile\" /    > \\n            \\n            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\\n            \\n            <a class=\"tag\" href=\"/tag/obvious/page/1/\">obvious</a>\\n            \\n            <a class=\"tag\" href=\"/tag/simile/page/1/\">simile</a>\\n            \\n        </div>\\n    </div>\\n\\n    <nav>\\n        <ul class=\"pager\">\\n            \\n            \\n            <li class=\"next\">\\n                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\\n            </li>\\n            \\n        </ul>\\n    </nav>\\n    </div>\\n    <div class=\"col-md-4 tags-box\">\\n        \\n            <h2>Top Ten tags</h2>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 28px\" href=\"/tag/love/\">love</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/inspirational/\">inspirational</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/life/\">life</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 24px\" href=\"/tag/humor/\">humor</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 22px\" href=\"/tag/books/\">books</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 14px\" href=\"/tag/reading/\">reading</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 10px\" href=\"/tag/friendship/\">friendship</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/friends/\">friends</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/truth/\">truth</a>\\n            </span>\\n            \\n            <span class=\"tag-item\">\\n            <a class=\"tag\" style=\"font-size: 6px\" href=\"/tag/simile/\">simile</a>\\n            </span>\\n            \\n        \\n    </div>\\n</div>\\n\\n    </div>\\n    <footer class=\"footer\">\\n        <div class=\"container\">\\n            <p class=\"text-muted\">\\n                Quotes by: <a href=\"https://www.goodreads.com/quotes\">GoodReads.com</a>\\n            </p>\\n            <p class=\"copyright\">\\n                Made with <span class=\\'sh-red\\'>\\xe2\\x9d\\xa4</span> by <a href=\"https://scrapinghub.com\">Scrapinghub</a>\\n            </p>\\n        </div>\\n    </footer>\\n</body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "class IronhackSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "\n",
    "\"\"\"\n",
    "This is a custom parser function you will complete in the challenge.\n",
    "Right now it simply returns the string passed to it. But in this lab\n",
    "you will complete this function so that it extracts the quotes.\n",
    "This function will be passed to the IronhackSpider class.\n",
    "\"\"\"\n",
    "def quotes_parser(content):\n",
    "    return content\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1 - Custom Parser Function\n",
    "\n",
    "In this challenge, complete the custom `quotes_parser()` function so that the returned result contains the quote string instead of the whole html page content.\n",
    "\n",
    "In the cell below, write your updated `quotes_parser()` function and kickstart the spider. Make sure the results being printed contain a list of quote strings extracted from the html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", '“Try not to become a man of success. Rather become a man of value.”', '“It is better to be hated for what you are than to be loved for what you are not.”', \"“I have not failed. I've just found 10,000 ways that won't work.”\", \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", '“A day without sunshine is like, you know, night.”']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    quotes = sopa.find(\"div\",{'class':'quote'}).findAllNext(\"span\",{'class':'text'})\n",
    "    clean_quotes = [i.text for i in quotes]\n",
    "    return clean_quotes\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2 - Error Handling\n",
    "\n",
    "In `IronhackSpider.scrape_url()`, catch any error that might occur when you make requests to scrape the webpage. This includes checking the response status code and catching http request errors such as timeout, SSL, and too many redirects.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful\n",
      "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", '“Try not to become a man of success. Rather become a man of value.”', '“It is better to be hated for what you are than to be loved for what you are not.”', \"“I have not failed. I've just found 10,000 ways that won't work.”\", \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", '“A day without sunshine is like, you know, night.”']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "class IronhackSpider:\n",
    "\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code < 300:\n",
    "                print('Request was successful')\n",
    "            elif response.status_code >= 400 and r.status_code < 500:\n",
    "                print('Request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('Request failed because the response server encountered an error')\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"It is taking too long, timeout error!\")\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"There was too many redirections, the url exceeded maximum redirects\")\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL not working, SSL Certificate Error\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Connection to url is not working, try again later!\")\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            \n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    quotes = sopa.find(\"div\",{'class':'quote'}).findAllNext(\"span\",{'class':'text'})\n",
    "    clean_quotes = [i.text for i in quotes]\n",
    "    return clean_quotes\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Sleep Interval\n",
    "\n",
    "In `IronhackSpider.kickstart()`, implement `sleep_interval`. You will check if `self.sleep_interval` is larger than 0. If so, tell the FOR loop to sleep the given amount of time before making the next request.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful\n",
      "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", '“Try not to become a man of success. Rather become a man of value.”', '“It is better to be hated for what you are than to be loved for what you are not.”', \"“I have not failed. I've just found 10,000 ways that won't work.”\", \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", '“A day without sunshine is like, you know, night.”']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import time\n",
    "\n",
    "class IronhackSpider:\n",
    "\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code < 300:\n",
    "                print('Request was successful')\n",
    "            elif response.status_code >= 400 and r.status_code < 500:\n",
    "                print('Request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('Request failed because the response server encountered an error')\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"It is taking too long, timeout error!\")\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"There was too many redirections, the url exceeded maximum redirects\")\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL not working, SSL Certificate Error\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Connection to url is not working, try again later!\")\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval>0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "SLEEP_INTERVAL = 3\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    quotes = sopa.find(\"div\",{'class':'quote'}).findAllNext(\"span\",{'class':'text'})\n",
    "    clean_quotes = [i.text for i in quotes]\n",
    "    return clean_quotes\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, SLEEP_INTERVAL, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Test Batch Scraping\n",
    "\n",
    "Change the `PAGES_TO_SCRAPE` value from `1` to `10`. Try if your code still works as intended to scrape 10 webpages. If there are errors in your code, fix them.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful\n",
      "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", '“Try not to become a man of success. Rather become a man of value.”', '“It is better to be hated for what you are than to be loved for what you are not.”', \"“I have not failed. I've just found 10,000 ways that won't work.”\", \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", '“A day without sunshine is like, you know, night.”']\n",
      "Request was successful\n",
      "[\"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\", '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', \"“If you can't explain it to a six year old, you don't understand it yourself.”\", \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\", '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”', '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”', \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”', '“Good friends, good books, and a sleepy conscience: this is the ideal life.”', '“Life is what happens to us while we are making other plans.”']\n",
      "Request was successful\n",
      "['“I love you without knowing how, or when, or from where. I love you simply, without problems or pride: I love you in this way because I do not know any other way of loving but this, in which there is no I or you, so intimate that your hand upon my chest is my hand, so intimate that when I fall asleep your eyes close.”', '“For every minute you are angry you lose sixty seconds of happiness.”', '“If you judge people, you have no time to love them.”', '“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”', '“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”', '“Today you are You, that is truer than true. There is no one alive who is Youer than You.”', '“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”', '“It is impossible to live without failing at something, unless you live so cautiously that you might as well not have lived at all - in which case, you fail by default.”', '“Logic will get you from A to Z; imagination will get you everywhere.”', '“One good thing about music, when it hits you, you feel no pain.”']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import time\n",
    "\n",
    "class IronhackSpider:\n",
    "\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code < 300:\n",
    "                print('Request was successful')\n",
    "            elif response.status_code >= 400 and r.status_code < 500:\n",
    "                print('Request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('Request failed because the response server encountered an error')\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"It is taking too long, timeout error!\")\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"There was too many redirections, the url exceeded maximum redirects\")\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL not working, SSL Certificate Error\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Connection to url is not working, try again later!\")\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval>0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 3 # how many webpages to scrapge\n",
    "SLEEP_INTERVAL = 3\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    quotes = sopa.find(\"div\",{'class':'quote'}).findAllNext(\"span\",{'class':'text'})\n",
    "    clean_quotes = [i.text for i in quotes]\n",
    "    return clean_quotes\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, SLEEP_INTERVAL, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Scrape a Different Website\n",
    "\n",
    "Update the parameters passed to the `IronhackSpider` constructor so that you coder can crawl [books.toscrape.com](http://books.toscrape.com/). You will need to use a different `URL_PATTERN` (figure out the new url pattern by yourself) and write another parser function to be passed to `IronhackSpider`. \n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful\n",
      "['A Light in the ...', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History ...', 'The Requiem Red', 'The Dirty Little Secrets ...', 'The Coming Woman: A ...', 'The Boys in the ...', 'The Black Maria', 'Starving Hearts (Triangular Trade ...', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little ...\", 'Rip it Up and ...', 'Our Band Could Be ...', 'Olio', 'Mesaerion: The Best Science ...', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n",
      "Request was successful\n",
      "['In Her Wake', 'How Music Works', 'Foolproof Preserving: A Guide ...', 'Chase Me (Paris Nights ...', 'Black Dust', 'Birdsong: A Story in ...', \"America's Cradle of Quarterbacks: ...\", 'Aladdin and His Wonderful ...', 'Worlds Elsewhere: Journeys Around ...', 'Wall and Piece', 'The Four Agreements: A ...', 'The Five Love Languages: ...', 'The Elephant Tree', 'The Bear and the ...', \"Sophie's World\", 'Penny Maybe', 'Maude (1883-1993):She Grew Up ...', 'In a Dark, Dark ...', 'Behind Closed Doors', \"You can't bury them ...\"]\n",
      "Request was successful\n",
      "['Slow States of Collapse: ...', 'Reasons to Stay Alive', 'Private Paris (Private #10)', '#HigherSelfie: Wake Up Your ...', 'Without Borders (Wanderlove #1)', 'When We Collided', 'We Love You, Charlie ...', 'Untitled Collection: Sabbath Poems ...', 'Unseen City: The Majesty ...', 'Unicorn Tracks', 'Unbound: How Eight Technologies ...', 'Tsubasa: WoRLD CHRoNiCLE 2 ...', 'Throwing Rocks at the ...', 'This One Summer', 'Thirst', 'The Torch Is Passed: ...', 'The Secret of Dreadwillow ...', 'The Pioneer Woman Cooks: ...', 'The Past Never Ends', 'The Natural History of ...']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import time\n",
    "\n",
    "class IronhackSpider:\n",
    "\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code < 300:\n",
    "                print('Request was successful')\n",
    "            elif response.status_code >= 400 and r.status_code < 500:\n",
    "                print('Request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('Request failed because the response server encountered an error')\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"It is taking too long, timeout error!\")\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"There was too many redirections, the url exceeded maximum redirects\")\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL not working, SSL Certificate Error\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Connection to url is not working, try again later!\")\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval>0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "URL_PATTERN = 'http://books.toscrape.com/catalogue/page-%s.html' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 3 # how many webpages to scrapge\n",
    "SLEEP_INTERVAL = 3\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    books = sopa.find(\"article\",{'class':'product_pod'}).findAllNext(\"h3\")\n",
    "    clean_books = [i.text for i in books]\n",
    "    return clean_books\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, SLEEP_INTERVAL, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 1 - Making Your Spider Unblockable\n",
    "\n",
    "Use techniques such as randomizing user agents and referers in your requests to reduce the likelihood that your spider is blocked by websites. [Here](http://blog.adnansiddiqi.me/5-strategies-to-write-unblock-able-web-scrapers-in-python/) is a great article to learn these techniques.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user-agent': 'Mozilla/5.0 (iPad; CPU OS 9_3_4 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13G35 Safari/601.1'}\n",
      "Request was successful\n",
      "['A Light in the ...', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History ...', 'The Requiem Red', 'The Dirty Little Secrets ...', 'The Coming Woman: A ...', 'The Boys in the ...', 'The Black Maria', 'Starving Hearts (Triangular Trade ...', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little ...\", 'Rip it Up and ...', 'Our Band Could Be ...', 'Olio', 'Mesaerion: The Best Science ...', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class IronhackSpider:\n",
    "\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        user_agent = get_random_ua()\n",
    "        headers = {'user-agent': user_agent}\n",
    "        print(headers)\n",
    "        try:\n",
    "            response = requests.get(url, headers = headers)\n",
    "            if response.status_code < 300:\n",
    "                print('Request was successful')\n",
    "            elif response.status_code >= 400 and r.status_code < 500:\n",
    "                print('Request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('Request failed because the response server encountered an error')\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"It is taking too long, timeout error!\")\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"There was too many redirections, the url exceeded maximum redirects\")\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL not working, SSL Certificate Error\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Connection to url is not working, try again later!\")\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval>0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "URL_PATTERN = 'http://books.toscrape.com/catalogue/page-%s.html' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "SLEEP_INTERVAL = 0\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    books = sopa.find(\"article\",{'class':'product_pod'}).findAllNext(\"h3\")\n",
    "    clean_books = [i.text for i in books]\n",
    "    return clean_books\n",
    "\n",
    "def get_random_ua():\n",
    "    random_ua = ''\n",
    "    ua_file = 'agents.txt'\n",
    "    try:\n",
    "        with open(ua_file) as f:\n",
    "            lines = f.readlines()\n",
    "        if len(lines) > 0:\n",
    "            prng = np.random.RandomState()\n",
    "            index = prng.permutation(len(lines) - 1)\n",
    "            idx = np.asarray(index, dtype=np.integer)[0]\n",
    "            random_ua = lines[int(idx)].replace(\"\\n\",\"\")\n",
    "    except Exception as ex:\n",
    "        print('Exception in random_ua')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return random_ua\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, SLEEP_INTERVAL, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Making Asynchronous Calls\n",
    "\n",
    "Implement asynchronous calls to `IronhackSpider`. You will make requests in parallel to complete your tasks faster.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empezando\n",
      "['http://books.toscrape.com/catalogue/page-1.html', 'http://books.toscrape.com/catalogue/page-2.html', 'http://books.toscrape.com/catalogue/page-3.html']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c8e335741aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Start scraping jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mmy_spider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkickstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_spider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkickstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com/catalogue/page-3.html\n",
      "http://books.toscrape.com/catalogue/page-1.html\n",
      "http://books.toscrape.com/catalogue/page-2.html\n",
      "Request was successful\n",
      "Request was successfulRequest was successful['A Light in the ...', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History ...', 'The Requiem Red', 'The Dirty Little Secrets ...', 'The Coming Woman: A ...', 'The Boys in the ...', 'The Black Maria', 'Starving Hearts (Triangular Trade ...', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little ...\", 'Rip it Up and ...', 'Our Band Could Be ...', 'Olio', 'Mesaerion: The Best Science ...', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n",
      "\n",
      "\n",
      "['Slow States of Collapse: ...', 'Reasons to Stay Alive', 'Private Paris (Private #10)', '#HigherSelfie: Wake Up Your ...', 'Without Borders (Wanderlove #1)', 'When We Collided', 'We Love You, Charlie ...', 'Untitled Collection: Sabbath Poems ...', 'Unseen City: The Majesty ...', 'Unicorn Tracks', 'Unbound: How Eight Technologies ...', 'Tsubasa: WoRLD CHRoNiCLE 2 ...', 'Throwing Rocks at the ...', 'This One Summer', 'Thirst', 'The Torch Is Passed: ...', 'The Secret of Dreadwillow ...', 'The Pioneer Woman Cooks: ...', 'The Past Never Ends', 'The Natural History of ...']['In Her Wake', 'How Music Works', 'Foolproof Preserving: A Guide ...', 'Chase Me (Paris Nights ...', 'Black Dust', 'Birdsong: A Story in ...', \"America's Cradle of Quarterbacks: ...\", 'Aladdin and His Wonderful ...', 'Worlds Elsewhere: Journeys Around ...', 'Wall and Piece', 'The Four Agreements: A ...', 'The Five Love Languages: ...', 'The Elephant Tree', 'The Bear and the ...', \"Sophie's World\", 'Penny Maybe', 'Maude (1883-1993):She Grew Up ...', 'In a Dark, Dark ...', 'Behind Closed Doors', \"You can't bury them ...\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import time\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        user_agent = get_random_ua()\n",
    "        headers = {'user-agent': user_agent}\n",
    "        print(url)\n",
    "        try:\n",
    "            response = requests.get(url, headers = headers)\n",
    "            if response.status_code < 300:\n",
    "                print('Request was successful')\n",
    "            elif response.status_code >= 400 and r.status_code < 500:\n",
    "                print('Request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('Request failed because the response server encountered an error')\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"It is taking too long, timeout error!\")\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"There was too many redirections, the url exceeded maximum redirects\")\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL not working, SSL Certificate Error\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Connection to url is not working, try again later!\")\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    async def kickstart(self):\n",
    "        print(\"empezando\")\n",
    "        urls = []\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            urls.append(self.url_pattern % i)\n",
    "        print(urls)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        await asyncio.sleep(1)\n",
    "        futures = [loop.run_in_executor(None, self.scrape_url, url) for url in urls]\n",
    "\n",
    "URL_PATTERN = 'http://books.toscrape.com/catalogue/page-%s.html' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 3 # how many webpages to scrapge\n",
    "SLEEP_INTERVAL = 0\n",
    "\n",
    "def quotes_parser(content):\n",
    "    sopa = bs(content, 'html.parser')\n",
    "    books = sopa.find(\"article\",{'class':'product_pod'}).findAllNext(\"h3\")\n",
    "    clean_books = [i.text for i in books]\n",
    "    return clean_books\n",
    "\n",
    "def get_random_ua():\n",
    "    random_ua = ''\n",
    "    ua_file = 'agents.txt'\n",
    "    try:\n",
    "        with open(ua_file) as f:\n",
    "            lines = f.readlines()\n",
    "        if len(lines) > 0:\n",
    "            prng = np.random.RandomState()\n",
    "            index = prng.permutation(len(lines) - 1)\n",
    "            idx = np.asarray(index, dtype=np.integer)[0]\n",
    "            random_ua = lines[int(idx)].replace(\"\\n\",\"\")\n",
    "    except Exception as ex:\n",
    "        print('Exception in random_ua')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return random_ua\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, SLEEP_INTERVAL, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "await my_spider.kickstart()\n",
    "asyncio.run(my_spider.kickstart()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
